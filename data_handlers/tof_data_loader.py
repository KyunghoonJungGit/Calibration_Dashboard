"""
Data Loader Module
Time of Flight ì‹¤í—˜ ë°ì´í„° íŒŒì¼ì„ ë¡œë“œí•˜ê³  ê²€ì¦í•˜ëŠ” ëª¨ë“ˆ
"""
import json
import xarray as xr
from pathlib import Path
from typing import Dict, Optional, Any, List
from datetime import datetime

class TOFDataLoader:
    """ì‹¤í—˜ ë°ì´í„° ë¡œë“œ ë° ê²€ì¦ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ë°ì´í„° ë¡œë” ì´ˆê¸°í™”"""
        # í•„ìˆ˜ íŒŒì¼ ëª©ë¡
        self.required_files = {
            'metadata': 'metadata.json',
            'ds_raw': 'ds_raw.nc',
            'ds_fit': 'ds_fit.nc',
            'qubit_info': 'qubit_info.json'
        }
        
        # ì„ íƒì  íŒŒì¼ ëª©ë¡
        self.optional_files = {
            'analysis_results': 'analysis_results.json',
            'experiment_config': 'experiment_config.json',
            'notes': 'notes.txt'
        }
        
        # ì§€ì›í•˜ëŠ” ì‹¤í—˜ íƒ€ì…
        self.supported_experiments = {
            'time_of_flight',
            'resonator_spectroscopy',
            'qubit_spectroscopy',
            'ramsey',
            'rabi_amplitude',
            'rabi_power',
            't1',
            't2',
            't2_echo'
        }
    
    def load_experiment(self, experiment_dir: Path) -> Optional[Dict]:
        """ì‹¤í—˜ í´ë”ì—ì„œ ë°ì´í„° ë¡œë“œ
        
        Parameters
        ----------
        experiment_dir : Path
            ì‹¤í—˜ ë°ì´í„°ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬
            
        Returns
        -------
        Dict or None
            ë¡œë“œëœ ì‹¤í—˜ ë°ì´í„° ë˜ëŠ” ì‹¤íŒ¨ ì‹œ None
        """
        try:
            # ë””ë ‰í† ë¦¬ ê²€ì¦
            if not experiment_dir.is_dir():
                raise ValueError(f"Not a directory: {experiment_dir}")
            
            # í•„ìˆ˜ íŒŒì¼ í™•ì¸
            self._validate_required_files(experiment_dir)
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            metadata = self._load_metadata(experiment_dir)
            
            # ì‹¤í—˜ íƒ€ì… ê²€ì¦
            exp_type = metadata.get('experiment_type')
            if exp_type not in self.supported_experiments:
                print(f"âš ï¸  Warning: Unknown experiment type '{exp_type}'")
            
            # xarray ë°ì´í„°ì…‹ ë¡œë“œ
            ds_raw = self._load_dataset(experiment_dir / self.required_files['ds_raw'])
            ds_fit = self._load_dataset(experiment_dir / self.required_files['ds_fit'])
            
            # Qubit ì •ë³´ ë¡œë“œ
            qubit_info = self._load_json(experiment_dir / self.required_files['qubit_info'])
            
            # ì„ íƒì  íŒŒì¼ ë¡œë“œ
            optional_data = self._load_optional_files(experiment_dir)
            
            # ë°ì´í„° í†µí•©
            experiment_data = {
                'type': exp_type,
                'timestamp': metadata.get('timestamp_full', metadata.get('timestamp')),
                'ds_raw': ds_raw,
                'ds_fit': ds_fit,
                'qubit_info': qubit_info,
                'metadata': metadata,
                **optional_data  # ì„ íƒì  ë°ì´í„° ì¶”ê°€
            }
            
            # ë°ì´í„° ê²€ì¦
            self._validate_experiment_data(experiment_data)
            
            return experiment_data
            
        except Exception as e:
            print(f"Error loading experiment from {experiment_dir}: {e}")
            return None
    
    def _validate_required_files(self, experiment_dir: Path):
        """í•„ìˆ˜ íŒŒì¼ ì¡´ì¬ í™•ì¸"""
        missing_files = []
        
        for file_type, filename in self.required_files.items():
            file_path = experiment_dir / filename
            if not file_path.exists():
                missing_files.append(filename)
        
        if missing_files:
            raise FileNotFoundError(
                f"Missing required files in {experiment_dir.name}: {', '.join(missing_files)}"
            )
    
    def _load_metadata(self, experiment_dir: Path) -> Dict:
        """ë©”íƒ€ë°ì´í„° ë¡œë“œ ë° ê²€ì¦"""
        metadata_path = experiment_dir / self.required_files['metadata']
        metadata = self._load_json(metadata_path)
        
        # í•„ìˆ˜ ë©”íƒ€ë°ì´í„° í•„ë“œ í™•ì¸
        required_fields = ['experiment_id', 'experiment_type', 'timestamp']
        missing_fields = [f for f in required_fields if f not in metadata]
        
        if missing_fields:
            raise ValueError(f"Missing required metadata fields: {', '.join(missing_fields)}")
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì‹± (í•„ìš”ì‹œ)
        if 'timestamp_full' not in metadata and 'timestamp' in metadata:
            try:
                # íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ ì¶”ì • ë° íŒŒì‹±
                timestamp_str = metadata['timestamp']
                if len(timestamp_str) == 15:  # YYYYMMDD_HHMMSS í˜•ì‹
                    dt = datetime.strptime(timestamp_str, "%Y%m%d_%H%M%S")
                    metadata['timestamp_full'] = dt.isoformat()
            except:
                pass
        
        return metadata
    
    def _load_dataset(self, file_path: Path) -> xr.Dataset:
        """xarray ë°ì´í„°ì…‹ ë¡œë“œ"""
        try:
            ds = xr.open_dataset(file_path)
            
            # ë°ì´í„°ì…‹ ê¸°ë³¸ ê²€ì¦
            if len(ds.data_vars) == 0:
                raise ValueError(f"Empty dataset: {file_path.name}")
            
            return ds
            
        except Exception as e:
            raise ValueError(f"Failed to load dataset {file_path.name}: {e}")
    
    def _load_json(self, file_path: Path) -> Dict:
        """JSON íŒŒì¼ ë¡œë“œ"""
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
            return data
        except json.JSONDecodeError as e:
            raise ValueError(f"Invalid JSON in {file_path.name}: {e}")
        except Exception as e:
            raise ValueError(f"Failed to load {file_path.name}: {e}")
    
    def _load_optional_files(self, experiment_dir: Path) -> Dict:
        """ì„ íƒì  íŒŒì¼ ë¡œë“œ"""
        optional_data = {}
        
        for file_type, filename in self.optional_files.items():
            file_path = experiment_dir / filename
            if file_path.exists():
                try:
                    if filename.endswith('.json'):
                        optional_data[file_type] = self._load_json(file_path)
                    elif filename.endswith('.txt'):
                        with open(file_path, 'r') as f:
                            optional_data[file_type] = f.read()
                    print(f"   ğŸ“„ Loaded optional: {filename}")
                except Exception as e:
                    print(f"   âš ï¸  Failed to load optional {filename}: {e}")
        
        return optional_data
    
    def _validate_experiment_data(self, experiment_data: Dict):
        """ë¡œë“œëœ ì‹¤í—˜ ë°ì´í„° ê²€ì¦"""
        # ë°ì´í„°ì…‹ ì°¨ì› ì¼ì¹˜ í™•ì¸
        ds_raw = experiment_data['ds_raw']
        ds_fit = experiment_data['ds_fit']
        qubit_info = experiment_data['qubit_info']
        
        # Qubit ì°¨ì› í™•ì¸
        if 'qubit' in ds_raw.dims:
            n_qubits_raw = len(ds_raw.qubit)
            n_qubits_info = len(qubit_info.get('grid_locations', []))
            
            if n_qubits_raw != n_qubits_info:
                print(f"âš ï¸  Warning: Qubit count mismatch - "
                      f"Dataset: {n_qubits_raw}, Info: {n_qubits_info}")
        
        # ì‹¤í—˜ íƒ€ì…ë³„ ì¶”ê°€ ê²€ì¦
        exp_type = experiment_data['type']
        self._validate_experiment_specific(exp_type, experiment_data)
    
    def _validate_experiment_specific(self, exp_type: str, experiment_data: Dict):
        """ì‹¤í—˜ íƒ€ì…ë³„ íŠ¹ìˆ˜ ê²€ì¦"""
        ds_raw = experiment_data['ds_raw']
        ds_fit = experiment_data['ds_fit']
        
        if exp_type == 'time_of_flight':
            required_vars = ['adcI', 'adcQ']
            missing_vars = [v for v in required_vars if v not in ds_raw.data_vars]
            if missing_vars:
                print(f"âš ï¸  Warning: Missing expected variables for TOF: {missing_vars}")
            
            if 'delay' not in ds_fit.data_vars:
                print("âš ï¸  Warning: 'delay' not found in fit results")
        
    def get_experiment_summary(self, experiment_data: Dict) -> Dict:
        """ì‹¤í—˜ ë°ì´í„° ìš”ì•½ ì •ë³´ ìƒì„±"""
        metadata = experiment_data['metadata']
        ds_raw = experiment_data['ds_raw']
        qubit_info = experiment_data['qubit_info']
        
        summary = {
            'experiment_id': metadata['experiment_id'],
            'experiment_type': metadata['experiment_type'],
            'timestamp': metadata.get('timestamp_full', metadata['timestamp']),
            'n_qubits': len(qubit_info.get('grid_locations', [])),
            'data_shape': dict(ds_raw.dims),
            'data_vars': list(ds_raw.data_vars),
            'coordinates': list(ds_raw.coords),
            'optional_data': list(k for k in experiment_data.keys() 
                               if k not in ['type', 'timestamp', 'ds_raw', 
                                          'ds_fit', 'qubit_info', 'metadata'])
        }
        
        return summary
    
    def save_experiment_summary(self, experiment_dir: Path, summary: Dict):
        """ì‹¤í—˜ ìš”ì•½ ì •ë³´ ì €ì¥ (ìºì‹±ìš©)"""
        summary_path = experiment_dir / 'summary.json'
        try:
            with open(summary_path, 'w') as f:
                json.dump(summary, f, indent=2)
            print(f"   ğŸ’¾ Saved experiment summary")
        except Exception as e:
            print(f"   âš ï¸  Failed to save summary: {e}")
    
    @staticmethod
    def validate_dataset_compatibility(ds1: xr.Dataset, ds2: xr.Dataset) -> bool:
        """ë‘ ë°ì´í„°ì…‹ì˜ í˜¸í™˜ì„± í™•ì¸ (ë¹„êµ ë¶„ì„ìš©)"""
        # ì°¨ì› í™•ì¸
        if ds1.dims.keys() != ds2.dims.keys():
            return False
        
        # ì¢Œí‘œ í™•ì¸
        for coord in ds1.coords:
            if coord not in ds2.coords:
                return False
            if not ds1[coord].equals(ds2[coord]):
                return False
        
        return True